# MengLa 行业数据面板 — 系统韧性改造分析

> 编写日期：2026-02-10
> 背景：系统 Docker 部署后，由于外部采集服务（extract.b.nps.qzsyzn.com）极不稳定，用户点击「采集」后长时间阻塞、超时、熔断，导致面板基本不可用。

---

## 一、现状架构概览

```
用户浏览器
  ↓ (3 分钟超时)
Nginx (proxy_read_timeout 300s)
  ↓
FastAPI 后端
  ├─ L1 本地缓存 (TTLCache, 5min)
  ├─ L2 Redis 缓存 (按颗粒度 TTL)
  ├─ L3 MongoDB 持久化
  └─ 外部采集服务 (HTTP → executionId → webhook 回调 → Redis 轮询)
       ↑
       MAX_INFLIGHT_REQUESTS = 1（串行）
       默认超时 300s
       熔断器：5 次失败 → OPEN 60s
```

**数据流**：前端 `queryMengla()` → 后端 `query_mengla()` → 三级缓存查找 → 未命中则调用外部采集 → 等待 webhook 回调（轮询 Redis）→ 落库 → 返回。

---

## 二、已修复的问题

| 问题 | 修复内容 |
|------|----------|
| Webhook 心跳污染 Redis | `webhook_routes.py`：跳过 `running/sync/pending/queued` 状态，只写入完成结果 |
| 轮询读到脏数据 | `client.py`：检测到 `running` 状态数据时删除并继续等待 |

---

## 三、核心问题分析

### 3.1 用户请求同步阻塞外部采集（最严重）

**现象**：用户点击「采集」→ 前端发 POST → 后端缓存未命中 → 调用外部采集 → 轮询等待 webhook 最长 300s → 前端 3 分钟超时 → 显示加载失败。

**根因**：`query_mengla()` 是同步阻塞模型——缓存未命中时，用户请求线程直接等待外部采集完成。外部服务不稳定时，这个等待几乎必然超时。

**影响**：
- 用户体验极差：点击后 3-5 分钟无响应
- `MAX_INFLIGHT_REQUESTS=1` 导致所有请求串行排队，一个卡住全部卡住
- 超时 5 次后熔断器 OPEN，后续请求直接报错

### 3.2 无降级策略——缓存未命中即死等

**现象**：趋势数据（`industryTrendRange`）跳过 L1/L2 缓存，直接查 L3 MongoDB。如果 MongoDB 中也没有，就必须走外部采集。

**根因**：`query_mengla()` 中趋势接口的缓存逻辑：
```python
if use_cache and not is_trend:
    cached_data, cache_source = await cache_manager.get(...)
```
趋势接口完全绕过了 `CacheManager` 的 L1/L2 层，直接在 `_fetch_mengla_data` 内部查 MongoDB。

**影响**：趋势数据是用户最常查看的，却是缓存命中率最低的。

### 3.3 熔断器恢复太慢、粒度太粗

**现状配置**：
- `failure_threshold = 5`：5 次失败即熔断
- `timeout = 60s`：熔断后 60s 才进入半开
- `success_threshold = 3`：半开状态需 3 次成功才恢复
- `half_open_max_calls = 3`：半开状态最多 3 个探测请求

**问题**：
- 外部服务不稳定时，5 次超时（每次 300s）= 25 分钟后才熔断，太晚了
- 熔断后 60s 才允许探测，但探测请求本身又可能超时 300s
- 熔断器按 `mengla_{action}` 分组，一个 action 熔断不影响其他，但同一 action 下所有类目都被阻断

### 3.4 前端无进度反馈、无缓存数据展示

**现象**：用户点击「采集」后只看到 loading spinner，没有任何进度信息。3 分钟后直接显示失败。

**问题**：
- 前端 `fetchTrigger` 机制：切换页面后 `fetchTrigger` 重置为 0，必须重新点击采集
- 没有「先展示缓存数据，后台刷新」的机制
- 没有采集进度条或预估时间

### 3.5 定时任务与用户请求竞争同一信号量

**现象**：定时任务（`scheduler.py`）和用户手动采集都调用 `query_mengla()`，共享 `MAX_INFLIGHT_REQUESTS=1` 的全局信号量。

**影响**：如果定时任务正在采集，用户请求必须排队等待。定时任务遍历所有类目 × 所有 action × 所有颗粒度，可能持续数小时。

---

## 四、改造方案

### 方案 A：异步采集 + 立即返回缓存（优先级：🔴 最高）

**核心思想**：用户请求永远不阻塞在外部采集上。缓存有数据就返回，没有就返回空 + 触发后台异步采集。

**改造点**：

1. **`query_mengla()` 拆分为两个模式**：
   - `sync=False`（默认，用户请求）：只查缓存，未命中返回 `(None, "miss")` 并异步触发采集
   - `sync=True`（定时任务/手动补齐）：保持现有阻塞行为

2. **新增后台采集队列**：
   - 缓存未命中时，将采集任务推入 Redis 队列（或内存队列）
   - 后台 worker 消费队列，完成后写入三级缓存
   - 前端轮询或 WebSocket 通知数据就绪

3. **前端改造**：
   - 首次加载：请求后端，后端返回缓存数据（可能为空）+ `collecting: true/false` 标记
   - 如果 `collecting=true`，前端显示「数据采集中...」并定时轮询
   - 数据就绪后自动刷新

**预期效果**：用户请求 < 1s 返回，不再有 3 分钟等待。

### 方案 B：趋势数据启用 L1/L2 缓存（优先级：🟠 高）

**改造点**：

1. 移除 `query_mengla()` 中 `if use_cache and not is_trend` 的限制
2. 趋势数据的缓存 key 设计：`mengla:data:industryTrendRange:{cat_id}:{granularity}:{start}_{end}`
3. L1 TTL 可以短一些（2-3 分钟），L2 按颗粒度 TTL

**预期效果**：趋势数据二次访问秒返回，大幅减少外部采集调用。

### 方案 C：优化熔断器参数（优先级：🟠 高）

**建议配置**：

```python
CIRCUIT_BREAKER_CONFIG = {
    "failure_threshold": 3,        # 3 次失败即熔断（原 5）
    "success_threshold": 2,        # 2 次成功即恢复（原 3）
    "timeout": 30,                 # 30s 后探测（原 60）
    "half_open_max_calls": 1,      # 半开状态只放 1 个探测（原 3）
}
```

**额外改造**：
- 熔断器探测请求使用更短的超时（如 30s 而非 300s）
- 熔断时返回缓存数据（如果有），而非直接报错
- 新增 `CircuitBreakerError` 的降级处理：查 MongoDB 返回历史数据

### 方案 D：缩短外部采集超时 + 快速失败（优先级：🟡 中）

**改造点**：

1. 用户请求的采集超时从 300s 降到 60s（环境变量 `MENGLA_TIMEOUT_SECONDS`）
2. 定时任务保持 300s（后台不着急）
3. 前端超时从 3 分钟降到 30s（配合方案 A，用户请求不应该等这么久）

### 方案 E：提升 MAX_INFLIGHT 并发度（优先级：🟡 中）

**现状**：`MAX_INFLIGHT_REQUESTS=1`，所有采集请求串行。

**建议**：
- 提升到 `MAX_INFLIGHT_REQUESTS=3`（需确认外部采集服务是否支持并发）
- 如果外部服务确实只能串行，考虑为用户请求和定时任务分配独立的信号量

### 方案 F：前端 stale-while-revalidate 策略（优先级：🟡 中）

**改造点**：

1. React Query 已有 `staleTime` 配置（当前 5 分钟），但 `fetchTrigger` 机制导致切换页面后数据丢失
2. 移除 `fetchTrigger` 重置逻辑，改为：
   - 页面加载时自动展示缓存数据（`enabled: !!primaryCatId`，去掉 `fetchTrigger > 0` 条件）
   - 「采集」按钮改为「刷新」，触发 `invalidateQueries` 强制重新请求
3. 增加 `gcTime`（当前 1 小时）保留更久的缓存

**预期效果**：用户切换页面不再看到空白，历史数据立即展示。

### 方案 G：缓存预热 + 定时任务优化（优先级：🟢 低）

**改造点**：

1. 服务启动时调用 `warmup_cache()` 预热热点数据到 L1/L2
2. 定时任务优先采集用户最近访问过的类目（基于访问日志或 Redis 计数）
3. 定时任务失败时不影响用户请求的熔断器状态（使用独立的熔断器实例）

---

## 五、实施优先级与工作量估算

| 优先级 | 方案 | 工作量 | 效果 |
|--------|------|--------|------|
| 🔴 P0 | A. 异步采集 + 立即返回 | 3-5 天 | 彻底解决用户阻塞问题 |
| 🟠 P1 | B. 趋势数据启用 L1/L2 | 0.5 天 | 趋势页二次访问秒返回 |
| 🟠 P1 | C. 优化熔断器参数 | 0.5 天 | 更快熔断、更快恢复 |
| 🟡 P2 | D. 缩短超时 + 快速失败 | 0.5 天 | 减少无效等待 |
| 🟡 P2 | E. 提升并发度 | 0.5 天 | 减少排队等待 |
| 🟡 P2 | F. 前端 stale-while-revalidate | 1-2 天 | 页面切换不再空白 |
| 🟢 P3 | G. 缓存预热 + 定时任务优化 | 1 天 | 冷启动体验提升 |

---

## 六、快速见效方案（可立即实施）

以下改动不涉及架构变更，可以通过修改配置或少量代码立即部署：

### 6.1 调整环境变量（零代码改动）

```env
# docker/.env.production 追加或修改
MENGLA_TIMEOUT_SECONDS=60          # 用户请求超时从 300s 降到 60s
CB_FAILURE_THRESHOLD=3             # 熔断阈值从 5 降到 3
CB_TIMEOUT=30                      # 熔断恢复探测从 60s 降到 30s
CB_SUCCESS_THRESHOLD=2             # 恢复所需成功次数从 3 降到 2
MAX_INFLIGHT_REQUESTS=2            # 并发从 1 提到 2（需确认外部服务支持）
```

### 6.2 熔断降级返回缓存数据（少量代码改动）

在 `query_mengla()` 的 `CircuitBreakerError` 处理中，不直接抛异常，而是尝试返回 MongoDB 历史数据：

```python
except CircuitBreakerError as e:
    # 熔断时降级：尝试返回 MongoDB 历史数据
    if not is_trend:
        fallback = await cache_manager.l3.get(action, _cat_id, _granularity, _period_key)
        if fallback and fallback.get("data"):
            logger.info("Circuit open, fallback to L3: action=%s", action)
            return (fallback["data"], "l3_fallback")
    raise
```

### 6.3 趋势数据启用 CacheManager（少量代码改动）

移除 `query_mengla()` 中 `not is_trend` 的限制，让趋势数据也走 L1/L2 缓存。

---

## 七、长期架构演进方向

1. **事件驱动架构**：采集完成后通过 WebSocket/SSE 推送前端，替代前端轮询
2. **采集任务队列化**：所有采集请求入队，后台 worker 异步处理，前端订阅结果
3. **数据新鲜度标记**：返回数据时附带 `freshness` 字段（`fresh` / `cached` / `stale`），前端据此展示提示
4. **外部服务健康监控**：独立的健康检查探针，提前感知外部服务状态，主动切换到降级模式
5. **多数据源容灾**：如果外部采集服务有备用节点，支持自动切换

---

## 八、总结

当前系统的核心矛盾是：**用户请求同步依赖一个极不稳定的外部服务**。所有改造的核心目标是打破这个同步依赖：

- **短期**（立即）：调参 + 熔断降级 + 趋势缓存，让系统在外部服务不可用时仍能返回历史数据
- **中期**（1-2 周）：异步采集 + 前端 stale-while-revalidate，用户请求永不阻塞
- **长期**：事件驱动 + 队列化，彻底解耦用户请求与外部采集
